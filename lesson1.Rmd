---
title: "Lesson 1 - The Basics"
output: html_document
---

```{r include=FALSE}
# In 2024, the first two sections (Intro to Bayesian stats, intro to computations) took ~45 mins each, the third section (Intro to Stan) took about 30 minutes
```

## Overall course approach

- Your own project to pass
- Too many exercises --- you likely won't complete all in class
  - There's always a minimal level of completion assumed for the next lesson --- you don't need to complete all (but it obviously helps)


## Basics of Bayesian Inference

- Probability distributions
  - Measure-theoretic objects
  - Extracting information from probability distributions via expectation values (see [Mike Betancourt's writing](https://betanalpha.github.io/assets/case_studies/probabilistic_computation.html) for a _very_ detailed coverage)
  - Can be represented as:
    - Cumulative distribution function
    - Probability density/mass function
    - Random number generator
  - Different representations useful for different tasks:
    - Conditioning easier with densities
    - Expectation values easier with random number generator (samples)

- Bayes theorem for events
  - Exercise - which terms should go into nominator/denumerator?
- Bayes theorem for models

\begin{gather*}
    \pi_\text{joint}(y, \theta) = \pi_\text{obs}(y | \theta) \pi_\text{prior}(\theta)\\
    \pi_\text{marg}\left(y \right) = \int_\Theta \mathrm{d} \theta \: \pi_{\text{obs}}(y | \theta) \pi_\text{prior}(\theta)\\
    \pi_\text{post}(\theta | y) = \frac{\pi_\text{obs}(y | \theta) \pi_\text{prior}(\theta)}{\pi_\text{marg}\left(y \right)}.
\end{gather*}


- Bayesian statistics $\neq$ Bayesian epistemology
  - Problems with catchall hypothesis
  - Problems in computation of Bayes factors
  - Problems with selective inference, early stopping
  - Compare to modern falsificationist ideas, notably Mayo (e.g. [Mayo 2018, SIST](https://www.cambridge.org/core/books/statistical-inference-as-severe-testing/D9DF409EF568090F3F60407FF2B973B2) --- I can lend you a copy, or [Mayo & Spanos 2011](https://doi.org/10.1016/B978-0-444-51862-0.50005-8))
    - Liftoff
- Great intro to the underlying philosophical ideas: Chapters 1 - 2 of McElreath's "Statistical Rethinking" - available freely online at https://xcelab.net/rmpubs/sr2/statisticalrethinking2_chapters1and2.pdf
  - McElreath also has great free course on Bayesian statistics at https://github.com/rmcelreath/stat_rethinking_2024
      
    

- MCMC algorithms allow us to sample posterior
  - We express our model as density (conditioning "easy") and get a random number generator for the posterior (expectations "easy")
  - What is a _chain_
  - Convergence, Rhat, ESS
- Hamiltonian Monte Carlo (HMC) - show example, requires gradient
  - A neat online demo: https://chi-feng.github.io/mcmc-demo/app.html?algorithm=HamiltonianMC&target=banana
     - Stan uses a HMC variant known as No U-turn sampler (NUTS)
  - Conceptually two components --- random initial momentum for each trajectory 
    makes the chain explore, preference for high density regions makes the chain spend more time there.
  - Wax poetic how HMC is great
  - HMC will work great, if gradient is useful
  - HMC has good diagnostic - the "divergence"
  

- What is needed to be proficient at probabilistic programming?
  - Mathematical probability/statistics
  - Numerics
  - A little software engineering

- Modelling notation, e.g.

$$
y \sim N(\mu,\sigma) \\
\mu \sim N(0, 1) \\
\sigma \sim HalfN(0, 2)
$$

- Modelled vs. unmodelled data

## Basics of Stan

- A decent tutorial covering similar ground as we did (and some more) is at https://betanalpha.github.io/assets/case_studies/stan_intro.html Note that the example R code there uses the `rstan` package, while we will use the newer `cmdstanr` package.
- Stan documentation: 
  - [language reference](https://mc-stan.org/docs/reference-manual/index.html) - how the language and algorithms work
  - [function reference](https://mc-stan.org/docs/functions-reference/index.html) - individual functions available in the language
  - [user's guide](https://mc-stan.org/docs/stan-users-guide/index.html) - worked out implementations of many model classes 
- Stan has very helpful community at https://discourse.mc-stan.org
- Fundamentally a Stan program computes logarithm of the density (and its gradient, Hessian via autodiff)
  - Why logarithms?
- Stan is statically typed (and generally quite restrictive). Discuss pros and cons.
- Stan architecture: 
  - Math library + autodiff
  - The language
  - The interfaces
  - compiler to C++, compile C++
- Go through the ["hello world" model](stan/lesson1/initial.stan) for the lesson.
- Main program blocks
  - `data` (both modelled and non-modelled)
  - `parameters`
  - `model`
- `target += ` statements
- `_lpdf`

## The tasks

- Start a project to contain your coursework
- In general: we expect you to use online documentation etc. a lot, just like
in "real life"
- [Task description](lesson1-tasks.html) - separate page

## Summary after tasks

- Constraints
- Divergences as helpful diagnostics
- Discuss project proposals
